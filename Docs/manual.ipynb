{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Bayesian Experimental Design\n",
    "\n",
    "R. D. McMichael \n",
    "rmcmichael@nist.gov  \n",
    "National Institute of Standards and Technology  \n",
    "Gaithersburg, MD  USA\n",
    "March 29, 2019\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This manual describes an implementation of optimal Bayesian experimental design methods.  These methods address routine measurements where data are fit to experimenal models in order to obtain model parameters.  The twin benefits of these methods  are reduced uncertainty with fewer required measurements.  These methods are therefore most beneficial in measurements where measurments are expensive in terms of money, time, risk, labor and/or discomfort.  The price for these benefits lies in the complexity of automating such measuremnts and in the computational load required.  It is the goal of this package to assist potential users in overcoming at least the programming hurdles.\n",
    "\n",
    "Optimal Bayesian experimental design is not new, at least not in the statistics community.  A review paper from 1995 by [Kathryn Chaloner and Isabella Verinelli](https://projecteuclid.org/euclid.ss/1177009939) reveals that the basic methods had been worked out in preceding decades.  The methods implemented here closely follow [Xun Huan and Youssef M. Marzouk](http://dx.doi.org/10.1016/j.jcp.2012.08.013) which emphasizes simulation-based experimental design.  Optimal Bayesian experimental design is also an active area of research\n",
    "\n",
    "There are at least three important factors that encourage application of these methods today.  First, the availability of flexible, modular (package friendly?) computer languages such as Python.  Second, availability of cheap computational power.   Most of all though, an increased awareness of the benefits of code sharing and reuse is growing in scientific communities, and the sharing is facilitated by websites such as sourceforge, bitbucket and github.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Philosophy and goals\n",
    "\n",
    "> If it sounds good, it is good\n",
    ">> Duke Ellington\n",
    "\n",
    "Of course, jazz legend Duke Ellington was talking about music, where it's all about the sound.  For this package, it's all about being useful.  The goals are modest: to adapt some of the developments in optimal Bayeseian experimental design research for practical use in laboratory settings, giving users tools to make better measurements. \n",
    "\n",
    "- If its a struggle to use, it can't run good.\n",
    "- If its to full of technical jargon to understand, it can't run good\n",
    "- If the user finds it useful, it runs good.\n",
    "- If it runs good, it is good.\n",
    "\n",
    "## Requirements for users\n",
    "\n",
    "It takes a little effort to get this software up and running.  Here's what a user will need to supply to get started.\n",
    "\n",
    "1. An experiment that yields measurement results with uncertainty estimates. \n",
    "2. A model for the experiment - typically a function with parameters to be determined. \n",
    "3. A working knowledge of Python programming - enough to follow examples and program your own model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gallery\n",
    "\n",
    "### Locating a Lorentzian peak\n",
    "\n",
    "\n",
    "\n",
    "Figure 1:  A comparison of measure-then-fit (left) and optimal Bayesian experimental design (right).  Both methods measure the same \"true\" peak with Gaussian noise added ($\\sigma = 1$) independently.  The peak parameters are selected randomly: center between 2 and 4, height between 2 and 5, background between -1 and 1 and peak width between 0.1 and 0.3.  On the left, 30 evenly-spaced \"measurements\" are made and fit using `scipy.curve_fit()`.  A curve usingthe best-fit parameters is plotted for comparison with the true curve.  The diagonal element of the covariance matrix is taken as the square of the uncertainty.   On the right, the optimal Bayes experimental design method is used sequentially.  Iterations stop when the standard deviation of the `x0` peak center distribution is less than the uncertaintly of the fit on the left.  Green curves correspond to random draws from the parameter distribution at the stopping point. Typical runs of this example require something like 1/4 to 1/2 as many measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning a $\\pi$ pulse\n",
    "![2D settings space](./pipulsefig2.png \"pi pulse demo\")\n",
    "\n",
    "Figure 2: A $\\pi$ pulse is a method of inverting spins that is frequenly used in nuclear magnetic resonance (NMR and MRI) and pulsed electron paramagnetic resonance (EPR).  In order to be accurate, the duration and frequency of the pulse must be tuned.  On the left, the background image displays the model photon counts for optically detected spin manipulation for different detunings and pulse lengths.  White indicates the expected result for spin up and black, spin down.  Points indicate simulated measurement settings, with sequence in order from white to dark red.  Simulated measurements have 1$\\sigma$ uncertainties of 100.  The right panel shows the probability distribution function after 50 measurements, with the red dot at the simulated \"true\" value.  The grey line shows the path of the probability maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slope Intercept\n",
    "![Straight line measurement examples](./slopeintercept.png \"slope intercept demo\")\n",
    "\n",
    "Figure 3: This demo uses a straight line model, a case where we know ahead of time where the \"best\" measurements should be made.  Measurements at the ends of a line are the most effective at reducing uncertainty in the slope and intercept values.  But perhaps we also need to be convinced or reassured that the straight line model is appropriate, and we'd like to have some data in the middle, too. The OptBayesExpt class provides two methods for flexibility in measurement selection.  The `opt_setting()` method selects the setting with the highest _utility_ $\\max[U(x)]$.  The first panel shows that it behaves as expected, choosing measurements at the ends of the line.  The `good_setting()` method is more flexible, selecting settings with a probability based on the _utility_ and the `pickiness` parameter.  The 2nd through 4th panels show that the `good_setting()` algorithm selects more diverse setting values as the `pickiness` is reduced.  Note also that the standard deviations increase from left to right as measurement resources are diverted away from reducing uncertainty.  Each run uses 40 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory of operation\n",
    "\n",
    "The optimal Bayes experimental design method incorporates two main jobs, which we can describe as \"learning fast\" and \"making good decisions\"\n",
    "\n",
    "### Learning fast\n",
    "\n",
    "The learning process is a straightforward application of the well-known Bayesian inference method.  If that last sentence made perfect sense to you, feel free to skip ahead.  \n",
    "\n",
    "We do measurements in order to learn things.  At the very beginning, before we start measuring, we may have some knowledge or experience, but we expect to have better information after we measure. Let's start in the logical place, ~~at the beginning~~ in the middle, asking how the knowledge is refined as a new measurment result is digested.  For that we need to use some technical language.\n",
    "\n",
    "We'll express our knowledge of the set of model parameters $\\theta$ as a probability distribution function $p(\\theta)$.  If $p(\\theta)$ is a broad distribution, then we really don't know the values very well, and if $p(\\theta)$ is narrow, the uncertaintly is small.  New measurement results $m$ should allow us to refine $p(\\theta)$; we want to know the new probability distribution $p(\\theta|m)$ after we have taken $m$ into account.  The vertical bar in the notation $p(\\theta|m)$ indicates a conditional probablility, the distribution of $\\theta$ values given $m$. \n",
    "\n",
    "Bayes theorem gives us\n",
    "  $$ p(\\theta|m) = \\frac{p(m|\\theta) p(\\theta)}{p(m)}. $$\n",
    "\n",
    "It's easy to write down Bayes theorem. on the other hand, understanding what we just wrote down can involve some conceptual hurdles, so let's pick it apart.   All of the terms here have technical names.  The left side is the _posterior_ distribution, i.e. the distribution of parameters $\\theta$ after we include $m$. On the right, distribution $p(\\theta)$ is the _prior_, representing what we knew about the parameters $\\theta$ before the measurement. In the denominator, $p(m)$ is the _evidence_, which has no $\\theta$ dependence.\n",
    "\n",
    "The term that we need to focus on, $p(m|\\theta)$, is called the _likelihood_.  It's the probability of getting the particular  results $m$ at settings $x$ given variable parameter values $\\theta$.  This is where we put our model to work. Given $x$ and $\\theta$, a supremely complete model would include not only the theory of our sample's behavior, but also the theory of the measurement noise, environemental fluctuations, the universe and everything.  It's a bit much to ask.  More typically, we have a model for a single model value $y_m(x,\\theta)$, and we supplement that with a phenomenological (i.e. measured) model for the measurement noise, environmental fluctuations, the universe and everything, namely, our experimental unertainty estimates.  Unpacking a mean value $\\bar{y}$ and uncertaintly $\\sigma_y$ from $m$, we can suppplement the simple model with the measured uncertainty to yield\n",
    "\n",
    "  $$ p(\\bar{y}|\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma_y} \\exp\\left[-\\frac{[\\bar{y} - y_m(x, \\theta)]^2 }{ 2\\sigma_y^2 } \\right]$$\n",
    "  \n",
    "Now we know how to update our \"knowledge\" of parameters $\\theta$ expressed as a probability distribution $P(\\theta)$.\n",
    "1. Collect measurement data including settings, $x$, measurement values $\\bar{y}$ and measurement uncertainties $\\sigma_y$.\n",
    "2. For all parameter combinations $\\theta$ calculate the model's prediction of the mean measurment result at setting $x$, $y_m(x, \\theta)$\n",
    "3. For all parameter combinations $\\theta$ multiply $p(\\theta)$ by the likelihood $\\exp[-(y-\\bar{y}(x, \\theta))^2/2\\sigma_y^2 ]$ \n",
    "\n",
    "We just made several important assumptions:\n",
    " - That our model function $y_m(x, \\theta)$ is a good description of reality\n",
    " - that the noise in our measurement is Gaussian with standard deviation $\\sigma_y$.\n",
    "\n",
    "On one hand we have to admit that these assumptions don't allow us to address all important cases.  On the other hand, these are the same assumptions we make in doing least-squares curve fitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making good decisions\n",
    "\n",
    "The next important job in the process is figuring out the settings for the next measurement that will best advance our goals.   At least part of our goal is to make the parameter probability distribution $p(\\theta)$ narrow while minimizing cost or time spent.  But we might have more than one goal: of course to find the best parameter values, but maybe also to convince a skeptical journal editor or to get project funding.\n",
    "\n",
    "The challenge, then is to develop a _utility function_ $U(x)$ that helps us to predict and compare the relative benefit/cost ratio of different possible experimental settings $x$.\n",
    "\n",
    "#### Estimate benefits\n",
    "\n",
    "First, an appeal to intuition.  Our system model describes a connection between parameter values $a$ and measurement results $y$.  Different measurements, e.g. from different samples, will yield different parameter values.  Similarly, different parameter values predict different measurements.  So, intuitively, if we want to constrain the parameter values, it would do the most good to \"pin down\" the measurement at the settings $x$ where the predicted variations in $y$ are the largest.  Measure where the the predicted results are most strongly coupled to the parameters $a$.  Our approach to making good decisions about measurement parameters goes like this:\n",
    "1. For random draws $\\theta_i$ of parameters from the distribution $p(\\theta)$\n",
    "   Use the model to predict $y_i(x)$ for every possible setting $x$.\n",
    "2. Calculate a measure of the spread in $y_i$ values\n",
    "3. Pick a measurement setting with a large spread.\n",
    "\n",
    "To translate such a qualitative argument into code, a good place to start is to clarify what we mean by \"doing the most good\" in refining our parameter distribution $p(\\theta)$.  When we determine model parameters, usually the goal is to get results with small uncertainty.  But here we're thinking in terms of a distribution $p(\\theta)$.  Information theory gives us information entropy as a way to quantify the sharpness of a probability distribution.  The information entropy of a probability distribution $p(a)$ is defined as  \n",
    " $$ E = -\\int da\\; p(a)\\; \\ln[p(a)] $$  \n",
    "Note that the integrand is zero for both $p(a) = 1$ and $p(a)=0$.  It's the intermediate values encountered in a spread-out distribution where the information entropy accumulates.  For common distributions, like rectangular or Gaussian, that have characteristic widths $w$ the entropy goes like $\\ln(w) + C$.\n",
    "\n",
    "We adopt the information entropy as our measure of $p(\\theta)$ sharpness, and that makes it possible to estimate how much $E$(posterior) - $E$(prior) we might get for predicted measurement values $y$ at different settings $x$.  Actually, the statisticians use something slightly different called the Kulback-Liebler divergence. \n",
    "$$ D^{KL} = \\int\\int d\\theta dy\\; p(\\theta |y,x)\\ln \\left[ \\frac{p(\\theta | y,x)}{p(\\theta)}\\right] $$  \n",
    "Here, we're using $y,x$ to denote predicted measurement results at potential settings $x$ instead of the $m$ we used for a completed measurement in the learning section above.\n",
    "\n",
    "The result for each potential setting $x$ is the difference between two information entropy values:\n",
    "1. The entropy of the $y$ distribution due to both measurement uncertainty and uncertainty in $\\theta$.\n",
    "  $$ \\int dy\\; p(y|x) \\ln[p(y|x)] $$\n",
    "  with\n",
    "  $$ p(y|x) = \\int d\\theta'\\; p(\\theta') p(y|\\theta',x) $$\n",
    "2. The entropy of the $y$ distribution due to measurement uncertainty alone, averaged over $\\theta$ values. \n",
    "  $$ \\int d\\theta\\; p(\\theta) \\int dy\\; p(y|\\theta,x) \\ln [ p(y|\\theta, x) ] $$\n",
    "\n",
    "This information entropy difference is the estimated improvement in the $\\theta$ distribution for different setting values, and the best setting choice for the next measurement is the one that maximizes the information change.\n",
    "\n",
    "If we were to implement this result directly, the next step would be to estimate the entropy integrals using random draws from $p(\\theta)$. To do this, we would need to make model measurement results for enough samples from $p(\\theta)$ to make a good estimate of $p(y|\\theta,x)$, and that task is computationally intensive.  It takes a lot of samples to get smooth distributions.\n",
    "\n",
    "In keeping with our \"runs good\" philosophy, let's consider _approximate methods_ (which is fancy talk for \"cheating.\")  What are the risks?  All we require of our decision-making algorithm is that is gives us smart, data-driven decisions. Is it critical that we make the absolute best measurement every single time? Probably not.  We don't need precise values, we just need to compare the utility of different settings.  Even if we don't choose the absolute best setting, a \"pretty good\" choice will do more good than an uninformed choice.  The only really bad possibility is the risk that the software will run too slowly to be useful.   \n",
    "\n",
    "Having given ourselves permission to be a little sloppy, let's approximate and see what happens if we assume all of the distributions are normal (Gaussian).  The information entropy of the normal distribution has a term that goes like $\\ln$(width).  Term 1 from above is a convolution of the measurement noise distribution (width = $\\sigma_y$ and the distribution of model $y$ values (width = $\\sigma_{y,\\theta}$) that reflects the connection to the parameter distribution.  A property of normal distributions is that a convolution of normal distributions is another normal distribution with width = $\\sqrt{\\sigma_{y,\\theta}^2 + \\sigma_y^2}$.  Under the assumption of normal distributions, we now have an approximate utility function\n",
    "\n",
    " $$ U^*(x) \\approx \\ln(\\sqrt{\\sigma_\\theta^2 + \\sigma_y^2}) - \\ln(\\sigma_y) \n",
    "         = \\frac{1}{2}\\ln\\left[\\frac{\\sigma_{y,\\theta}(x)^2}{\\sigma_y(x)^2}+1\\right]$$\n",
    "\n",
    "This approximation has some reasonable properties. The dependence on $\\sigma_{y,\\theta}$ matches our initial intuition that high-utility parameters are those where measurements vary a lot due to parameter variations.  The dependence on measurement noise $\\sigma_y$ also has an intuitive interpretation: that it's less useful to make measurements at settings $x$ where the instrumental noise is larger.  This approximate utility function is also positive, i.e. more data helps narrow a distribution.   \n",
    "Finally, $U^*(x)$, which is an approximate information entropy change, has the property that the parameter distribution width $\\sigma_\\theta$ behaves asymptoticaly like $N^{-1/2}$ after $N$ iterations when noise dominates.  At least in one dimension, \n",
    "  $$ d \\ln(\\sigma_\\theta) = \\frac{1}{2}\\frac{\\sigma_{y,\\theta}(x)^2}{\\sigma_y(x)^2}dN. $$\n",
    "The left side is the approximate entropy change in one iteration ($dN = 1$).  If the parameter variations are small enough, $\\sigma_{y,\\theta} \\approx dy/d\\theta\\; \\sigma_\\theta$.  Then the differential equation implied above has the solution\n",
    "  $$ \\sigma_\\theta \\propto N^{-1/2} $$\n",
    "which is typical \"beating down the noise\" behavior of long-term averaging.\n",
    "\n",
    "\n",
    "#### Estimate the costs\n",
    "\n",
    "There are two very important questions that we have left unresolved:\n",
    "1. What if some settings are more difficult/time consuming/expensive than others?\n",
    "2. When should I quit measuring?\n",
    "\n",
    "Both of these questions involve weighing the cost of a measurement against the expected benefits, and that's something that we just can't do, because a) we don't know how and b) it'd be a legal mess if we claimed that we did.   Economic decisions are left to the user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing pieces\n",
    "\n",
    "A. In some situations, the model predicts a distribution of measured values, separate from the measurement uncertainty.  Quantum mechanics for example. How do we handle those?\n",
    "\n",
    "B. So far, measurement noise enters with measurement data, consistent with the notion that all measurments should be provided with uncertainty values.  \n",
    "\n",
    "C. How do we handle situations with multiple measurements at once, like voltage and current, each with its own uncertainty?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
